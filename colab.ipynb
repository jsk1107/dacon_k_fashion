{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "colab.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsk1107/dacon_k_fashion/blob/master/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "6NVI84w4nWla"
      },
      "source": [
        "## Colab Session 연장"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymFuGYCpnWlb"
      },
      "source": [
        "### 아래 코드를 F12를 누른 후 Console에 입력할것 \n",
        "\n",
        "function ClickConnect() {\n",
        "    var buttons = document.querySelectorAll(\"colab-dialog.yes-no-dialog paper-button#cancel\"); \n",
        "    buttons.forEach(function(btn) { \n",
        "        btn.click(); \n",
        "    }); \n",
        "    console.log(\"1분마다 자동 재연결\"); \n",
        "    document.querySelector(\"colab-toolbar-button#connect\").click(); \n",
        "} \n",
        "setInterval(ClickConnect,1000*60);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_2bwC4wrOEP"
      },
      "source": [
        "# CLI 환경 실행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGXEin_Forr0",
        "outputId": "8c59f47c-b42d-4c70-9208-789c28525dcd"
      },
      "source": [
        "!pip install kora\r\n",
        "from kora import console\r\n",
        "console.start()  # and click link"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kora\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/0d/3d9ab9ee747f0925b038e8350ce137276a7a4730a96a3516485dc1b87ba3/kora-0.9.19-py3-none-any.whl (57kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 17.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20kB 18.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30kB 14.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51kB 9.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from kora) (5.5.0)\n",
            "Collecting fastcore\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0c/98/60404e2817cff113a6ae4023bc1772e23179408fdf7857fa410551758dfe/fastcore-1.3.19-py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (5.0.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (54.0.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->kora) (2.6.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from fastcore->kora) (20.9)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastcore->kora) (19.3.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kora) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kora) (1.15.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->kora) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->kora) (0.7.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->fastcore->kora) (2.4.7)\n",
            "Installing collected packages: fastcore, kora\n",
            "Successfully installed fastcore-1.3.19 kora-0.9.19\n",
            "Console URL: https://teleconsole.com/s/21cb506a7b355bf756b49afbe1942d575c84307e\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw1HujV_uZxV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da0451e8-67ff-4b08-dc28-c42af75c11e9"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Mar  7 08:37:46 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.39       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    33W / 250W |  15965MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbJno14hnWlb"
      },
      "source": [
        "!git clone https://github.com/jsk1107/dacon_k_fashion"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WYtZPXM0nWlc",
        "outputId": "330ea173-6dec-48bc-ac07-559a6620050a"
      },
      "source": [
        "!wget dacon-datasets.s3.ap-northeast-2.amazonaws.com/k-fashion/data.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-07 08:21:11--  http://dacon-datasets.s3.ap-northeast-2.amazonaws.com/k-fashion/data.zip\n",
            "Resolving dacon-datasets.s3.ap-northeast-2.amazonaws.com (dacon-datasets.s3.ap-northeast-2.amazonaws.com)... 52.219.58.107\n",
            "Connecting to dacon-datasets.s3.ap-northeast-2.amazonaws.com (dacon-datasets.s3.ap-northeast-2.amazonaws.com)|52.219.58.107|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5306730245 (4.9G) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   4.94G  24.2MB/s    in 3m 33s  \n",
            "\n",
            "2021-03-07 08:24:44 (23.8 MB/s) - ‘data.zip’ saved [5306730245/5306730245]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqTyK2CCojbl"
      },
      "source": [
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VoR9LTSrI9t"
      },
      "source": [
        "!pip install -r /content/dacon_k_fashion/requrirement.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAGVOnm3sTM-"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN6yIKaAsUmt"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torchvision.models.detection import retinanet_resnet50_fpn\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "from torchvision.transforms import transforms as T\r\n",
        "from torchvision.models.detection.retinanet import RetinaNetClassificationHead\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from pycocotools.coco import COCO\r\n",
        "import os\r\n",
        "import cv2\r\n",
        "from PIL import Image\r\n",
        "import numpy as np\r\n",
        "from pycocotools.cocoeval import COCOeval"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCtVRJn40EuM"
      },
      "source": [
        "!pip list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4nFFuCTsJa2"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_wzQD9uqv0w"
      },
      "source": [
        "class KfashionDataset(Dataset):\r\n",
        "    def __init__(self, img_dir, anno_dir, set_name='train', transforms=None):\r\n",
        "        self.img_dir = img_dir\r\n",
        "        self.anno_dir = anno_dir\r\n",
        "        self.transforms = transforms\r\n",
        "\r\n",
        "        self.coco = COCO(os.path.join(self.anno_dir, f'{set_name}.json'))\r\n",
        "\r\n",
        "        self.image_ids = self.coco.getImgIds()\r\n",
        "        self.category = self.coco.loadCats(self.coco.getCatIds())\r\n",
        "\r\n",
        "    def __getitem__(self, ids):\r\n",
        "\r\n",
        "        img = self.get_image(ids)\r\n",
        "        target = self.get_annotaion(ids)\r\n",
        "\r\n",
        "        if self.transforms is not None:\r\n",
        "            img = self.transforms(img)\r\n",
        "\r\n",
        "        return img, target\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.image_ids)\r\n",
        "\r\n",
        "    def get_image(self, ids):\r\n",
        "        img_info = self.coco.loadImgs(self.image_ids[ids])[0]\r\n",
        "        key = list(img_info['file_name'])[0]\r\n",
        "        img_path = os.path.join(self.img_dir, key, img_info['file_name'])\r\n",
        "        img = Image.open(img_path)\r\n",
        "        # img = cv2.imread(img_path)\r\n",
        "\r\n",
        "        return img\r\n",
        "\r\n",
        "    def get_annotaion(self, ids):\r\n",
        "        anno_ids = self.coco.getAnnIds(imgIds=self.image_ids[ids], iscrowd=False)\r\n",
        "        coco_annos = self.coco.loadAnns(anno_ids)\r\n",
        "        target = {}\r\n",
        "        boxes = []\r\n",
        "        labels = []\r\n",
        "        for coco_anno in coco_annos:\r\n",
        "            bbox = self.bbox_transform(coco_anno['bbox'])\r\n",
        "            boxes.append(bbox)\r\n",
        "            labels.append(coco_anno['category_id']-1)\r\n",
        "        target['boxes'] = torch.as_tensor(boxes, dtype=torch.float32)\r\n",
        "        target['labels'] = torch.as_tensor(labels, dtype=torch.int64)\r\n",
        "\r\n",
        "        return target\r\n",
        "\r\n",
        "    def bbox_transform(self, bbox):\r\n",
        "        bbox[2] = bbox[0] + bbox[2]\r\n",
        "        bbox[3] = bbox[1] + bbox[3]\r\n",
        "        return bbox"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjzmrMRQsNzH"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZIML99bsOet",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19f23e95-6d41-4608-a3a3-e424baa8deeb"
      },
      "source": [
        "def collate_fn(batch):\r\n",
        "    return tuple(zip(*batch))\r\n",
        "\r\n",
        "def get_transform(train):\r\n",
        "    transforms = []\r\n",
        "    transforms.append(T.ToTensor())\r\n",
        "    if train:\r\n",
        "        # (역자주: 학습시 50% 확률로 학습 영상을 좌우 반전 변환합니다)\r\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\r\n",
        "    return T.Compose(transforms)\r\n",
        "\r\n",
        "\r\n",
        "img_dir = '/content/train/'\r\n",
        "anno_dir = '/content/'\r\n",
        "set_name = 'train'\r\n",
        "batch_size = 20\r\n",
        "dataset = KfashionDataset(img_dir, anno_dir, set_name, transforms=get_transform(True))\r\n",
        "num_classes = len(dataset.category)\r\n",
        "print(f'num_classes: {num_classes}')\r\n",
        "\r\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)\r\n",
        "\r\n",
        "model = retinanet_resnet50_fpn(True)\r\n",
        "num_anchor = model.anchor_generator.num_anchors_per_location()[0]\r\n",
        "in_channels = model.head.classification_head.cls_logits.in_channels\r\n",
        "model.head.classification_head = RetinaNetClassificationHead(in_channels, num_anchor, num_classes)\r\n",
        "\r\n",
        "optim = torch.optim.SGD(model.parameters(), lr=1e-4)\r\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n",
        "\r\n",
        "model.train()\r\n",
        "for images, targets in dataloader:\r\n",
        "    optim.zero_grad()\r\n",
        "\r\n",
        "    if torch.cuda.is_available():\r\n",
        "      model.to(device)\r\n",
        "      images = list(image.to(device) for image in images)\r\n",
        "      targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\r\n",
        "    out = model(images, targets)\r\n",
        "\r\n",
        "    cls_loss = out['classification']\r\n",
        "    reg_loss = out['bbox_regression']\r\n",
        "\r\n",
        "    loss = cls_loss + reg_loss\r\n",
        "    print(f'loss: {loss}')\r\n",
        "\r\n",
        "    loss.backward()\r\n",
        "    optim.step()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=1.89s)\n",
            "creating index...\n",
            "index created!\n",
            "num_classes: 21\n",
            "loss: 1.7252113819122314\n",
            "loss: 1.6827086210250854\n",
            "loss: 1.7344772815704346\n",
            "loss: 1.567409634590149\n",
            "loss: 1.6352978944778442\n",
            "loss: 1.70468008518219\n",
            "loss: 1.6469064950942993\n",
            "loss: 1.722879409790039\n",
            "loss: 1.6249010562896729\n",
            "loss: 1.7602455615997314\n",
            "loss: 1.6725921630859375\n",
            "loss: 1.6606550216674805\n",
            "loss: 1.654381275177002\n",
            "loss: 1.6604841947555542\n",
            "loss: 1.682253122329712\n",
            "loss: 1.7492671012878418\n",
            "loss: 1.6338809728622437\n",
            "loss: 1.690420389175415\n",
            "loss: 1.6196894645690918\n",
            "loss: 1.6687963008880615\n",
            "loss: 1.686237096786499\n",
            "loss: 1.643257737159729\n",
            "loss: 1.6679308414459229\n",
            "loss: 1.708317756652832\n",
            "loss: 1.6392338275909424\n",
            "loss: 1.6391501426696777\n",
            "loss: 1.6530126333236694\n",
            "loss: 1.6762075424194336\n",
            "loss: 1.631446361541748\n",
            "loss: 1.6850080490112305\n",
            "loss: 1.6424906253814697\n",
            "loss: 1.6656920909881592\n",
            "loss: 1.76228928565979\n",
            "loss: 1.590933918952942\n",
            "loss: 1.638733148574829\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-4e40ff14ef29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m       \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mcls_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'classification'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/detection/retinanet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    480\u001b[0m                         raise ValueError(\"Expected target boxes to be a tensor\"\n\u001b[1;32m    481\u001b[0m                                          \"of shape [N, 4], got {:}.\".format(\n\u001b[0;32m--> 482\u001b[0;31m                                              boxes.shape))\n\u001b[0m\u001b[1;32m    483\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m                     raise ValueError(\"Expected target boxes to be of type \"\n",
            "\u001b[0;31mValueError\u001b[0m: Expected target boxes to be a tensorof shape [N, 4], got torch.Size([0])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4xtPB_3snqH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}